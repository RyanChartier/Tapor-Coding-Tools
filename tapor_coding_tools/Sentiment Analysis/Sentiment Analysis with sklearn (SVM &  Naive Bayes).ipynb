{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis using Scikit-learn (SVM vs Naive Bayes)\n",
    "\n",
    "This Notebooks with conduct sentiment-analysis using publicly available data dump [Movie Review Data](http://www.cs.cornell.edu/people/pabo/movie-review-data/). We will compare performance of two Machine Learning techniques, [Support Vector Machines (SVM)](http://scikit-learn.org/stable/modules/svm.html) and [Naive Bayes](http://scikit-learn.org/stable/modules/naive_bayes.html) to see which one gives a more accurate _sentiment polarity_ (positive or negative)\n",
    "\n",
    "### Data sources\n",
    "**Training set:** [polarity dataset v1.1 (2.2Mb)](http://www.cs.cornell.edu/people/pabo/movie-review-data/mix20_rand700_tokens_0211.tar.gz) _includes a README_ – approximately 700 positive and 700 negative processed reviews. Released November 2002. This alternative version was created by Nathan Treloar, who removed a few non-English/incomplete reviews and changing some of the labels (judging some polarities to be different from the original author's rating). The complete list of changes made to v1.1 can be found in diff.txt.\n",
    "\n",
    "**Testing set:** [polarity dataset v0.9 (2.8Mb)](http://www.cs.cornell.edu/people/pabo/movie-review-data/mix20_rand700_tokens.zip) _includes a README_ – 700 positive and 700 negative processed reviews. Introduced in Pang/Lee/Vaithyanathan EMNLP 2002. Released July 2002.\n",
    "\n",
    "### Download\n",
    "Download and extract the above linked files preferably on the same directory as your iPython Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Methods\n",
    "\n",
    "Define two methods:\n",
    "\n",
    "  * `list_textfiles()` - Retrieves all the text files in a given directory\n",
    "  * `read_txt()` - Reading the contents of a given text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import codecs\n",
    "\n",
    "def list_textfiles(directory):\n",
    "    \"\"\"Return a list of filenames ending in '.txt'\n",
    "    directory - name of folder to look into\n",
    "    \"\"\"\n",
    "    textfiles = []\n",
    "    for filename in listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            textfiles.append(directory + \"/\" + filename)\n",
    "    return textfiles\n",
    "\n",
    "\n",
    "def read_txt(filename):\n",
    "    \"\"\"Return text content of a given file\n",
    "    filename - name of file to open and read\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with codecs.open(filename,'r',encoding='utf-8', errors='ignore') as f :\n",
    "            text = f.read()\n",
    "    except EnvironmentError: # parent of IOError, OSError *and* WindowsError where available\n",
    "        print(\"Oops! Couldn't read the given file\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "Import the training datasets into lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import training data\n",
    "filenames_pos = list_textfiles(\"movieReview_data/tokens/pos\")\n",
    "filenames_neg = list_textfiles(\"movieReview_data/tokens/neg\")\n",
    "\n",
    "# create two lists, one to store review text and one stores the polarity\n",
    "data_train = []\n",
    "data_labels_train = []\n",
    "\n",
    "for f in filenames_pos:\n",
    "    data_train.append(read_txt(f))\n",
    "    data_labels_train.append('pos')\n",
    "\n",
    "for f in filenames_neg:\n",
    "    data_train.append(read_txt(f))\n",
    "    data_labels_train.append('neg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "Next, we initialize a [sckit-learn](http://scikit-learn.org/stable/index.html) vector with the [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) class. Because the data could be in any format, we’ll set lowercase to False and exclude common words such as “the” or “and”. This vectorizer will transform our data into vectors of features. \n",
    "\n",
    "`min_df` - Used to set a threshold for the vocabulary that can be ignored. Giving a float of 0.5 implies getting the proportion of documents, giving an integer implies absolute count of documents.\n",
    "\n",
    "`max_df` - Used to set a threshold for vocabulary terms to ignore that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts.\n",
    "\n",
    "Once the TfidfVectorizer class is initialized, we fit it onto the data above and convert it to an array for easy usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=5,max_df=0.8, sublinear_tf=True,use_idf=True)\n",
    "features_train = vectorizer.fit_transform(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Classifier\n",
    "### SVM\n",
    "Using Support vector machines (SVM), we classify the training data using the polarity labels and create a model for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC()\n",
    "# train svm model\n",
    "clf.fit(features_train, data_labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "Load the dataset for testing. Import the dataset into lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np;\n",
    "\n",
    "# import test data\n",
    "filenames_pos_test = list_textfiles(\"mix20_rand700_tokens/tokens/pos\")\n",
    "filenames_neg_test = list_textfiles(\"mix20_rand700_tokens/tokens/neg\")\n",
    "\n",
    "# create two lists, one to store review text and one stores the polarity\n",
    "data_test = []\n",
    "data_labels_test = []\n",
    "\n",
    "for f in filenames_pos:\n",
    "    data_test.append(read_txt(f))\n",
    "    data_labels_test.append('pos')\n",
    "\n",
    "for f in filenames_neg:\n",
    "    data_test.append(read_txt(f))\n",
    "    data_labels_test.append('neg')\n",
    "\n",
    "# vectorize\n",
    "features_test = vectorizer.fit_transform(data_test)\n",
    "\n",
    "#features_nd_test = features_test.toarray() # for easy usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scores\n",
    "\n",
    "Obtain the accuracy scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of SVM model: 0.9385714285714286\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.97      0.91      0.94       700\n",
      "        pos       0.91      0.97      0.94       700\n",
      "\n",
      "avg / total       0.94      0.94      0.94      1400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "predicted = clf.predict(features_test)\n",
    "\n",
    "# print accuracy score\n",
    "print(\"Accuracy score of SVM model: {}\\n\".format(accuracy_score(data_labels_test,predicted)))\n",
    "# print evaluation report showing: precision, recall, f1-score, support\n",
    "print(classification_report(data_labels_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "\n",
    "We perform the same steps using Naive Bayes [MultinomialNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(features_train, data_labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of Naive Bayes model: 0.965\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.96      0.97      0.97       700\n",
      "        pos       0.97      0.96      0.96       700\n",
      "\n",
      "avg / total       0.97      0.96      0.96      1400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "mnb_predict = mnb.predict(features_test)\n",
    "\n",
    "print(\"Accuracy score of Naive Bayes model: {}\\n\".format(accuracy_score(data_labels_test,mnb_predict)))\n",
    "print(classification_report(data_labels_test, mnb_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dumping model\n",
    "\n",
    "joblib.dump(value, filename, compress=0, protocol=None, cache_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentNB.model']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(mnb,'sentNB.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "\n",
    "Why Naive Bayes performed much better than SVM in this prediction?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
