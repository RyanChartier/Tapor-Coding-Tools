{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TATR: Tokenization and Extraction\n",
    "\n",
    "This notebook is part of a greater series of Jupyter Notebooks structured around Twitter Tweet analysis. This particular notebook will look at tokenizing and extracting key features from a tweet. This notebook also serves as one of the introductory notebooks for TATR as tokenization and extraction are fundamental features for any analysis. \n",
    "\n",
    "Any additional assumptions and clarification will be discussed and declared throughout the notebook.\n",
    "\n",
    "### Note: \n",
    "This notebook recommends that you look at TATR: Panda and CSV of Tweets. This is because we will be using some of the features discussed in that notebook. \n",
    "\n",
    "Written 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: Tokenization\n",
    "\n",
    "Tokenization is the process by which we separate a string of text into its parts. Whether that be by words, sentences or some other rubric. Therefore, the tokenization is important when segmenting your Twitter data. This will be further elaborated on when tokenization comes up. \n",
    "\n",
    "To find out more about tokenization in general:\n",
    "* https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "Now we will import all the Python 3 libraries that will be used in this notebook. You do not need to know all the functionalities of each library as some are massive. However, any functionalities that are used will be explained as they appear, so do not worry too much if you do not recognize the libraries. \n",
    "\n",
    "To import or download the required libraries see the Jupyter documentation or the libraries' home page for instruction. \n",
    "\n",
    "### Note: \n",
    "All libraries that are used are available for Anaconda "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing data structure libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import text analysist tools\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization using NLTK\n",
    "\n",
    "In this notebook we will look at using NLTK (Natural Language Toolkit) for tokenization. NLTK is a popular and powerful Python library for working with human language data. The library contains many different functionalities from tokenization to word classification. In this notebook we will just be examining the tokenization features of NLTK.\n",
    "\n",
    "To find out more about NLTK see:\n",
    "* http://www.nltk.org/\n",
    "\n",
    "To find out more about NLTK tokenization see:\n",
    "* http://www.nltk.org/api/nltk.tokenize.html\n",
    "\n",
    "## Prefix\n",
    "\n",
    "In this notebook we will be creating 4 dummy tweets. This is done to ensure that anyone coming to this notebook is able to test the functionalities of it without needing a set of Twitter tweets before hand. If you do have a corpus already feel free to skip this section.\n",
    "\n",
    "### Note:\n",
    "\n",
    "This notebook uses Panda as it primary data structure and CSV as its data file format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is a basic tweet without anything speical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This tweet uses #twitter hashtags</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is used to reply to @user</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Combing different @uses of things! and http:/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0     This is a basic tweet without anything speical\n",
       "1                  This tweet uses #twitter hashtags\n",
       "2                     This is used to reply to @user\n",
       "3  #Combing different @uses of things! and http:/..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the panda dataframe\n",
    "pandaDataFrame = pd.DataFrame({ \n",
    "                                'Text' :[\"This is a basic tweet without anything speical\",\n",
    "                                         \"This tweet uses #twitter hashtags\",\n",
    "                                         \"This is used to reply to @user\",\n",
    "                                         \"#Combing different @uses of things! and http://www.google.ca :D\"]\n",
    "                              })\n",
    "\n",
    "# Lets see what the dataframe look like\n",
    "pandaDataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Now that we have some data loaded into a Panda dataframe, we can begin tokenization. NLTK offers a variety of methods for tokenization. In this notebook we will look at one particular tokenization NLTK offers, that is TweetTokenizer. This is NLTK tokenization made especially for tweets.\n",
    "\n",
    "We will first create a function to conduct the tokenization. The reason for this will be explained later on in the notebook. \n",
    "\n",
    "### Note: \n",
    "During tokenization we also record the amount of tokens created. Although not important for this particular notebook, having the count can be useful for analysis.\n",
    "\n",
    "There are also additional options that can be enabled with NLTK TweetTokenizer, however in this notebook we will not be using them.\n",
    "\n",
    "In addition, depending on your data set, this process can take a long time. Therefore, it is best to segment larger datasets into smaller subsets. The process of segmenting larger datasets into smaller ones can be found in a more advance notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tokenize the text within the dataframe\n",
    "\n",
    ":dataframe: The dataframe with the tweets\n",
    ":column_name: the column name with the tweet text\n",
    "\"\"\"\n",
    "def tokenize_text(dataframe, column_name):\n",
    "\n",
    "    # Initalize the tokenizer\n",
    "    tweetTokenizer = TweetTokenizer()\n",
    "    \n",
    "    # Calculate the amount of tokens\n",
    "    token = tweetTokenizer.tokenize(dataframe[column_name]) \n",
    "\n",
    "    # Save tokenized text into a new column called \"token\"\n",
    "    dataframe['token'] = token\n",
    "    \n",
    "    # Save token Count into a new column called \"count_token\"\n",
    "    dataframe['count_token'] = len(token)\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have it initalized we can apply this to each value of the dataframe. To do this we will examine a nice feature of Panda, \"apply\" and \"lamda\". Using \"apply\" it allows us to apply the function to each data cell in Text. \"Lamda\" allows us to run functions with mulitple parameters in apply.\n",
    "\n",
    "In the \"apply\" function, \"axis\" can either refer to '0' or rows or '1' or more columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>token</th>\n",
       "      <th>count_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is a basic tweet without anything speical</td>\n",
       "      <td>[This, is, a, basic, tweet, without, anything,...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This tweet uses #twitter hashtags</td>\n",
       "      <td>[This, tweet, uses, #twitter, hashtags]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is used to reply to @user</td>\n",
       "      <td>[This, is, used, to, reply, to, @user]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Combing different @uses of things! and http:/...</td>\n",
       "      <td>[#Combing, different, @uses, of, things, !, an...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0     This is a basic tweet without anything speical   \n",
       "1                  This tweet uses #twitter hashtags   \n",
       "2                     This is used to reply to @user   \n",
       "3  #Combing different @uses of things! and http:/...   \n",
       "\n",
       "                                               token  count_token  \n",
       "0  [This, is, a, basic, tweet, without, anything,...            8  \n",
       "1            [This, tweet, uses, #twitter, hashtags]            5  \n",
       "2             [This, is, used, to, reply, to, @user]            7  \n",
       "3  [#Combing, different, @uses, of, things, !, an...            9  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the tokenizer on our dataframe and save it into a new dataframe\n",
    "# In this case \"x\" refers to the \"pandaDataframe\" and \"Text\" is the column label\n",
    "TokenizeTweetFrame = pandaDataFrame.apply(lambda x: tokenize_text(x, \"Text\"), axis=1)\n",
    "\n",
    "# Seeing what TokenizeTweetFrame look like\n",
    "TokenizeTweetFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we now have 3 columns in our dataframe (excluding index). You can see that for each text, hashtags, replies, words, and emoji that they are separated into their own individual tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Extracting Token\n",
    "\n",
    "Now that we have each tweet tokenized, we can move on to extraction. Although having the tokenized version of each tweet is useful, you may only be concerned with one particular aspect of the tweet text. For example, hashtags are an interesting area to analyze. In this case, having another column with just the hashtags would be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Hashtags from Tokens\n",
    "\n",
    "We will first start out by extracting hashtags into their own column. To do so we are going to have to use some regular expressions, or regex. Regular expression is a method of matching a sequence of characters, in this case hashtags. Therefore, we will again declare a function, similar to the one before for tokenization, that extracts hashtags and their count. \n",
    "\n",
    "To find out more on regular expressions see:\n",
    "* https://en.wikipedia.org/wiki/Regular_expression\n",
    "\n",
    "For a quick reference on what regular expressions can do see:\n",
    "* https://docs.microsoft.com/en-us/dotnet/standard/base-types/regular-expression-language-quick-referencete:\n",
    "\n",
    "### Note:\n",
    "During extraction of the hashtags, we will be removing the \"#\" from the results. This is because we know all the results in the new column will be hashtags. It will be easier and cleaner to remove them during this process than later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the hashtags within the tokenized text\n",
    "\n",
    ":dataframe: The dataframe with the tweets\n",
    ":column_name: the column name with the token text\n",
    "\"\"\"\n",
    "def extract_hashtag(dataframe, column_name):\n",
    "    \n",
    "    # Finds all the hashtags with regex \n",
    "    # The regex matches all sequences that start with \"#\" \n",
    "    hashtag = re.findall(r\"#(\\S+)\", dataframe[column_name])\n",
    "\n",
    "    # Insert hashtag and count into dataframe\n",
    "    \n",
    "    # If there are any hashtags insert them and their count\n",
    "    if hashtag:\n",
    "        dataframe['HASHTAG'] = hashtag\n",
    "        dataframe['count_hashtag'] = len(hashtag)\n",
    "        \n",
    "    # If there are no hashtags just insert empty values\n",
    "    else:\n",
    "        dataframe['HASHTAG'] = []\n",
    "        dataframe['count_hashtag'] = 0\n",
    "        \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we will be using Panda's apply and lamda features to apply this function to the dataset. In addition, we will be expanding the previous dataframe \"TokenizeTweetFrame\" with the new columns. However, feel free to create a new dataframe for the extracted tokens and count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>token</th>\n",
       "      <th>count_token</th>\n",
       "      <th>HASHTAG</th>\n",
       "      <th>count_hashtag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is a basic tweet without anything speical</td>\n",
       "      <td>[This, is, a, basic, tweet, without, anything,...</td>\n",
       "      <td>8</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This tweet uses #twitter hashtags</td>\n",
       "      <td>[This, tweet, uses, #twitter, hashtags]</td>\n",
       "      <td>5</td>\n",
       "      <td>[twitter]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is used to reply to @user</td>\n",
       "      <td>[This, is, used, to, reply, to, @user]</td>\n",
       "      <td>7</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Combing different @uses of things! and http:/...</td>\n",
       "      <td>[#Combing, different, @uses, of, things, !, an...</td>\n",
       "      <td>9</td>\n",
       "      <td>[Combing]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0     This is a basic tweet without anything speical   \n",
       "1                  This tweet uses #twitter hashtags   \n",
       "2                     This is used to reply to @user   \n",
       "3  #Combing different @uses of things! and http:/...   \n",
       "\n",
       "                                               token  count_token    HASHTAG  \\\n",
       "0  [This, is, a, basic, tweet, without, anything,...            8         []   \n",
       "1            [This, tweet, uses, #twitter, hashtags]            5  [twitter]   \n",
       "2             [This, is, used, to, reply, to, @user]            7         []   \n",
       "3  [#Combing, different, @uses, of, things, !, an...            9  [Combing]   \n",
       "\n",
       "   count_hashtag  \n",
       "0              0  \n",
       "1              1  \n",
       "2              0  \n",
       "3              1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the extractor on our TokenizeTweetFrame\n",
    "# In this case \"x\" refers to the \"TokenizeTweetFrame\" and \"Text\" is the column label\n",
    "TokenizeTweetFrame = TokenizeTweetFrame.apply(lambda x: extract_hashtag(x, \"Text\"), axis=1)\n",
    "\n",
    "# Seeing what TokenizeTweetFrame look like\n",
    "TokenizeTweetFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have extracted the different hashtags from the text, we can do the same for other tokens as well. In this notebook we will showcase how to also extract urls and replies. However, in this notebook we will be moving them to their own dataframe. This is done mostly to keep the dataframe small and readable for this notebook format. Feel free to keep it as one dataframe.\n",
    "\n",
    "We will first start by writing a function that removes the replies. You will find it is very similar to the extract_hashtag function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the replies within the tokenized text\n",
    "\n",
    ":dataframe: The dataframe with the tweets\n",
    ":column_name: the column name with the token text\n",
    "\"\"\"\n",
    "def extract_replies(dataframe, column_name):\n",
    "    \n",
    "    # Finds all the replies with regex \n",
    "    # The regex matches all sequences that start with \"@\"\n",
    "    replies = re.findall(r\"@(\\S+)\", dataframe[column_name])\n",
    "    \n",
    "    # If there are any replies insert them and their count\n",
    "    if replies:\n",
    "        dataframe['REPLIES'] = replies\n",
    "        dataframe['count_replies'] = len(replies)\n",
    "        \n",
    "    # If there are no replies just insert empty values\n",
    "    else:\n",
    "        dataframe['REPLIES'] = []\n",
    "        dataframe['count_replies'] = 0\n",
    "        \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that we will be using the original pandaDataframe and not the TokenizeTweetFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>REPLIES</th>\n",
       "      <th>count_replies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is a basic tweet without anything speical</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This tweet uses #twitter hashtags</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is used to reply to @user</td>\n",
       "      <td>[user]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Combing different @uses of things! and http:/...</td>\n",
       "      <td>[uses]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text REPLIES  count_replies\n",
       "0     This is a basic tweet without anything speical      []              0\n",
       "1                  This tweet uses #twitter hashtags      []              0\n",
       "2                     This is used to reply to @user  [user]              1\n",
       "3  #Combing different @uses of things! and http:/...  [uses]              1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the extractor on our pandaDataFrame\n",
    "# In this case \"x\" refers to the \"pandaDataFrame\" and \"Text\" is the column label\n",
    "ExtractTweetFrame = pandaDataFrame.apply(lambda x: extract_replies(x, \"Text\"), axis=1)\n",
    "\n",
    "# Seeing what ExtractTweetFrame look like\n",
    "ExtractTweetFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final type of token we will extract are the URL links. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract the URLs within the tokenized text\n",
    "\n",
    ":dataframe: The dataframe with the tweets\n",
    ":column_name: the column name with the token text\n",
    "\"\"\"\n",
    "def extract_URL(dataframe, column_name):\n",
    "    \n",
    "    # Finds all the urls with regex \n",
    "    # The regex matches all sequences that start with \"http\" \n",
    "    URL = re.findall(r\"http\\S+\", dataframe[column_name])\n",
    "    \n",
    "    # If there are any urls insert them and their count\n",
    "    if URL:\n",
    "        dataframe['URL'] = URL\n",
    "        dataframe['count_URL'] = len(URL)\n",
    "        \n",
    "    # If there are no urls just insert empty values\n",
    "    else:\n",
    "        dataframe['URL'] = []\n",
    "        dataframe['count_URL'] = 0\n",
    "        \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we will be expanding ExtractTweetFrame in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>REPLIES</th>\n",
       "      <th>count_replies</th>\n",
       "      <th>URL</th>\n",
       "      <th>count_URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is a basic tweet without anything speical</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This tweet uses #twitter hashtags</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is used to reply to @user</td>\n",
       "      <td>[user]</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Combing different @uses of things! and http:/...</td>\n",
       "      <td>[uses]</td>\n",
       "      <td>1</td>\n",
       "      <td>[http://www.google.ca]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text REPLIES  count_replies  \\\n",
       "0     This is a basic tweet without anything speical      []              0   \n",
       "1                  This tweet uses #twitter hashtags      []              0   \n",
       "2                     This is used to reply to @user  [user]              1   \n",
       "3  #Combing different @uses of things! and http:/...  [uses]              1   \n",
       "\n",
       "                      URL  count_URL  \n",
       "0                      []          0  \n",
       "1                      []          0  \n",
       "2                      []          0  \n",
       "3  [http://www.google.ca]          1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the extractor on our pandaDataFrame\n",
    "# In this case \"x\" refers to the \"pandaDataFrame\" and \"Text\" is the column label\n",
    "ExtractTweetFrame = ExtractTweetFrame.apply(lambda x: extract_URL(x, \"Text\"), axis=1)\n",
    "\n",
    "# Seeing what ExtractTweetFrame look like\n",
    "ExtractTweetFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook we looked at tokenization using NLTK. The library allowed us to separate the tweet into different components to be extracted out into their own segement. Using Panda apply and lamda features we were able to extract out hashtags, replies, URL, and their counts into their own columns. \n",
    "\n",
    "Although this notebook does not go into detail on how to save these results (these will be explored in a different notebook), it does provide some of the foundational work that may be needed in one's research. Therefore, to keep this notebook from touching on too many topics, features such as cleaning are not discussed in this particular notebook but will be discussed in another.\n",
    "\n",
    "This notebook serves as one of the introductory notebook in the TATR series. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
